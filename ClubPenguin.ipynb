{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cb7905-22bb-4b0c-a9eb-b9be388b0d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ultralytics\n",
      "  Using cached ultralytics-8.1.11-py3-none-any.whl (709 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (1.24.3)\n",
      "Collecting opencv-python>=4.6.0 (from ultralytics)\n",
      "  Using cached opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (9.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (5.4.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (1.10.1)\n",
      "Collecting torch>=1.8.0 (from ultralytics)\n",
      "  Using cached torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
      "Collecting torchvision>=0.9.0 (from ultralytics)\n",
      "  Using cached torchvision-0.17.0-cp311-cp311-manylinux1_x86_64.whl (6.9 MB)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: psutil in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (5.9.5)\n",
      "Collecting py-cpuinfo (from ultralytics)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Collecting thop>=0.1.1 (from ultralytics)\n",
      "  Using cached thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (2.0.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from requests>=2.23.0->ultralytics) (2023.5.7)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.12.2)\n",
      "Collecting typing-extensions>=4.8.0 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting sympy (from torch>=1.8.0->ultralytics)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==2.2.0 (from torch>=1.8.0->ultralytics)\n",
      "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=1.8.0->ultralytics)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: py-cpuinfo, mpmath, typing-extensions, triton, sympy, opencv-python, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, thop, ultralytics\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.6.3\n",
      "    Uninstalling typing_extensions-4.6.3:\n",
      "      Successfully uninstalled typing_extensions-4.6.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xmip 0.7.1 requires xgcm<0.7.0, but you have xgcm 0.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 opencv-python-4.9.0.80 py-cpuinfo-9.0.0 sympy-1.12 thop-0.1.1.post2209072238 torch-2.2.0 torchvision-0.17.0 triton-2.2.0 typing-extensions-4.9.0 ultralytics-8.1.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.6 MB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from opencv-python-headless) (1.24.3)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.9.0.80\n",
      "Collecting labelme2yolo\n",
      "  Using cached labelme2yolo-0.1.4-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.23.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from labelme2yolo) (1.24.3)\n",
      "Requirement already satisfied: opencv-python>=4.1.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from labelme2yolo) (4.9.0.80)\n",
      "Requirement already satisfied: pillow<10.3,>=9.2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from labelme2yolo) (9.5.0)\n",
      "Installing collected packages: labelme2yolo\n",
      "Successfully installed labelme2yolo-0.1.4\n"
     ]
    }
   ],
   "source": [
    "# Run this cell only once.\n",
    "# %pip install ultralytics\n",
    "# !pip install opencv-python-headless\n",
    "# !pip install labelme2yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf09dd7-3f95-443a-8948-475b128fc1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# GeoTiff Images\n",
    "import rasterio\n",
    "from osgeo import gdal\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as img\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Model Building\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "import labelme2yolo\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4989e799-4c3e-4b23-9224-68bb6308be21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Import the data (Only need to run once)\n",
    "\n",
    "# 2a. Pre-event data\n",
    "# !wget https://challenge.ey.com/api/v1/storage/admin-files/Pre_Event_San_Juan.tif -O Pre_Event_San_Juan.tif\n",
    "\n",
    "# 2b.Post-event data\n",
    "# !wget https://challenge.ey.com/api/v1/storage/admin-files/Post_Event_San_Juan.tif -O Post_Event_San_Juan.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5918fc-7034-4fef-8f11-52f580955aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Assign the data files to variables \n",
    "# pre_event_image = './Pre_Event_San_Juan.tif'\n",
    "# post_event_image ='./Post_Event_San_Juan.tif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a6262-ebbe-4e6a-a236-e8909badda49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4. Data Preprocessing\n",
    "# # 4a. Create grid size 512x512 for the images (define the function)\n",
    "# def generate_tiles(input_file, output_dir,grid_x,grid_y):\n",
    "#     ds = gdal.Open(input_file)\n",
    "\n",
    "#     # Get image size and number of bands\n",
    "#     width = ds.RasterXSize\n",
    "#     height = ds.RasterYSize\n",
    "#     num_bands = ds.RasterCount\n",
    "\n",
    "#     # Calculate number of tiles in each dimension\n",
    "#     num_tiles_x = (width // grid_x)\n",
    "#     num_tiles_y = (height // grid_y)\n",
    "\n",
    "#     print(f\"Total number of tiles: {num_tiles_x * num_tiles_y}\")\n",
    "\n",
    "#     # Create output directory if it doesn't exist\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # Iterate over each tile and save as a separate TIFF image\n",
    "#     for i in range(num_tiles_x):\n",
    "#         for j in range(num_tiles_y):\n",
    "#             x_offset = i *  grid_x\n",
    "#             y_offset = j *  grid_y\n",
    "\n",
    "#             tile_width = min(grid_x, width - x_offset)\n",
    "#             tile_height = min(grid_y, height - y_offset)\n",
    "\n",
    "#             tile = []\n",
    "#             for band in range(1, num_bands + 1):\n",
    "#                 tile_data = ds.GetRasterBand(band).ReadAsArray(x_offset, y_offset, tile_width, tile_height)\n",
    "#                 tile.append(tile_data)\n",
    "\n",
    "#             # Create output filename\n",
    "#             output_file = os.path.join(output_dir, f\"tile_{i}_{j}.tif\")\n",
    "                \n",
    "#             # Create an output TIFF file with same CRS and band values range\n",
    "#             driver = gdal.GetDriverByName(\"GTiff\")\n",
    "#             options = ['COMPRESS=DEFLATE', 'PREDICTOR=2', 'TILED=YES']\n",
    "#             out_ds = driver.Create(output_file, tile_width, tile_height, num_bands, \n",
    "#                        ds.GetRasterBand(1).DataType, options=options)\n",
    "#             # out_ds = driver.Create(output_file, tile_width, tile_height, num_bands, ds.GetRasterBand(1).DataType)\n",
    "\n",
    "#             # Set the geotransform\n",
    "#             geotransform = list(ds.GetGeoTransform())\n",
    "#             geotransform[0] = geotransform[0] + x_offset * geotransform[1]\n",
    "#             geotransform[3] = geotransform[3] + y_offset * geotransform[5]\n",
    "#             out_ds.SetGeoTransform(tuple(geotransform))\n",
    "\n",
    "#             # Set the projection\n",
    "#             out_ds.SetProjection(ds.GetProjection())\n",
    "\n",
    "#             # Write each band to the output file\n",
    "#             for band in range(1, num_bands + 1):\n",
    "#                 out_band = out_ds.GetRasterBand(band)\n",
    "#                 out_band.WriteArray(tile[band - 1])\n",
    "\n",
    "#             # Close the output file\n",
    "#             out_ds = None\n",
    "\n",
    "#     print(\"Tiles generation completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7de642-a86d-475f-bc26-9158172ca780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4a. Create grid size 512x512 for the images (specify directory for pre event images)\n",
    "# input_file = \"./Pre_Event_San_Juan.tif\"\n",
    "# output_dir = \"./Pre_Event_Grids_In_TIFF\"\n",
    "# grid_x = 512\n",
    "# grid_y = 512\n",
    "# generate_tiles(input_file, output_dir,grid_x,grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f922e275-969e-49a3-b860-b4306b6fe55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4a. Create grid size 512x512 for the images (specify directory for post event images)\n",
    "# input_file = \"./Post_Event_San_Juan.tif\"\n",
    "# output_dir = \"./Post_Event_Grids_In_TIFF\"\n",
    "# grid_x = 512\n",
    "# grid_y = 512\n",
    "# generate_tiles(input_file, output_dir,grid_x,grid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc57501-56ba-418f-a580-84b3a80d24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4b. Convert TIF images into JPG (define the function)\n",
    "# def convert_tiff_to_jpeg(input_dir,output_dir):\n",
    "#     # check if output_dir exists, if not create it\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     for filename in os.listdir(input_dir):\n",
    "#         # check if file is an image (ends with .tif)\n",
    "#         if filename.endswith('.tif'):\n",
    "#             img = Image.open(os.path.join(input_dir, filename))\n",
    "        \n",
    "#             # check if image is RGB mode, if not convert it\n",
    "#             if img.mode != 'RGB':\n",
    "#                 img = img.convert('RGB')\n",
    "        \n",
    "#             # create new filename, replace .tif with .jpg\n",
    "#             output_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "        \n",
    "#             # save the image in JPEG format\n",
    "#             img.save(os.path.join(output_dir, output_filename), 'JPEG')\n",
    "#     print(\"Conversion from TIFF to JPEG completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08569e6-0ebc-4cce-b8cf-187044ee39a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4b. Convert TIF images into JPG (specify directory for pre event images)\n",
    "# pre_event_input_dir = \"./Pre_Event_Grids_In_TIFF\"\n",
    "# pre_event_output_dir = \"./Pre_Event_Grids_In_JPEG\"\n",
    "# convert_tiff_to_jpeg(pre_event_input_dir,pre_event_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f8fca3-cc83-4925-b0aa-ddeeafc93eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4b. Convert TIF images into JPG (specify directory for post event images)\n",
    "# post_event_input_dir = \"./Post_Event_Grids_In_TIFF\"\n",
    "# post_event_output_dir = \"./Post_Event_Grids_In_JPEG\"\n",
    "# convert_tiff_to_jpeg(post_event_input_dir,post_event_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf69fd-e3b6-4e49-9ea4-bba62c666c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4c. Rename files in the directory (define the function)\n",
    "# def rename_files(directory_path,prefix):\n",
    "# # Define the directory path where your files are located\n",
    "#     directory_path = directory_path\n",
    "    \n",
    "#     # Get a list of all files in the directory\n",
    "#     files = os.listdir(directory_path)\n",
    "    \n",
    "#     # Define a prefix for the new file names \n",
    "#     # Change the prefix as per requirement\n",
    "#     prefix = prefix\n",
    "    \n",
    "#     # Start the numbering from 1\n",
    "#     number = 0\n",
    "    \n",
    "#     # Loop through each file in the directory\n",
    "#     for filename in files:\n",
    "#         # Check if the item is a file (not a directory)\n",
    "#         if os.path.isfile(os.path.join(directory_path, filename)):\n",
    "#             # Get the file extension\n",
    "#             file_extension = os.path.splitext(filename)[1]\n",
    "    \n",
    "#             # Create the new file name with leading zeros\n",
    "#             new_filename = f\"{prefix}{number:03}{file_extension}\"\n",
    "    \n",
    "#             # Construct the full path to the original and new files\n",
    "#             old_filepath = os.path.join(directory_path, filename)\n",
    "#             new_filepath = os.path.join(directory_path, new_filename)\n",
    "    \n",
    "#             # Rename the file\n",
    "#             os.rename(old_filepath, new_filepath)\n",
    "    \n",
    "#             # Increment the number for the next file\n",
    "#             number += 1\n",
    "    \n",
    "#     print(\"Files renamed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce5020-f912-49e4-977f-28a4bbfcc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4c. Rename files in the directory (pre event images)\n",
    "# rename_files(pre_event_output_dir, \"Pre_Event_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfaadb-0a68-45d2-90a7-911811c03423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 4c. Rename files in the directory (post event images)\n",
    "# rename_files(pre_event_output_dir, \"Post_Event_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976a6208-120e-4420-be88-e4bc2edea212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. Load the annotated data\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# 5a. Create a new folder for the annotated data\n",
    "os.makedirs('Post_Event_Annotated_JSON', exist_ok=True)\n",
    "os.makedirs('Pre_Event_Annotated_JSON', exist_ok=True)\n",
    "\n",
    "def unzip_file(zip_filepath, dest_dir):\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as zip_ref:\n",
    "        zip_ref.extractall(dest_dir)\n",
    "\n",
    "# Usage\n",
    "unzip_file('./Post_Event_Manual_Annotated.zip', './Post_Event_Annotated_JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f741d5-a5b9-4812-8054-6c4f403369da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. Prepare the file to be YOLO-ready\n",
    "# 6a. Transform the .json file into .txt format (define the function)\n",
    "import os, json\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b52207c4-442c-4703-8402-8e25df4ebb99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:labelme2yolo:Converting train set ...\n",
      "100%|█████████████████████████████████████████████| 9/9 [00:01<00:00,  8.78it/s]\n",
      "INFO:labelme2yolo:Converting val set ...\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00,  8.55it/s]\n",
      "INFO:labelme2yolo:Converting test set ...\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# 6b. Generate the configuration file (dataset.yaml)\n",
    "!labelme2yolo --json_dir ./Post_Event_Annotated_JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fa8012-7c11-458a-b018-c036e81ec403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(225, 3157200, 0, 8.8575488)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. Build the model\n",
    "#Loading the YOLO model\n",
    "model = YOLO('yolov8n.pt')\n",
    "# Display model information (optional)\n",
    "model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2feebe2f-95c6-42bb-93a7-1fc5c807fd81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.11.4 torch-2.2.0+cu121 CPU (Intel Xeon Platinum 8272CL 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset.yaml, epochs=50, time=None, patience=50, batch=16, imgsz=512, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train63, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train63\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'dataset.yaml' error ❌ \nDataset 'dataset.yaml' images not found ⚠️, missing path '/home/jovyan/PlanetaryComputerExamples/datasets/val'\nNote dataset download directory is '/home/jovyan/PlanetaryComputerExamples/datasets'. You can update this in '/home/jovyan/.config/Ultralytics/settings.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/trainer.py:133\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myml\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata:\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/data/utils.py:327\u001b[0m, in \u001b[0;36mcheck_det_dataset\u001b[0;34m(dataset, autodownload)\u001b[0m\n\u001b[1;32m    326\u001b[0m     m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNote dataset download directory is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATASETS_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. You can update this in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSETTINGS_YAML\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(m)\n\u001b[1;32m    328\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \nDataset 'dataset.yaml' images not found ⚠️, missing path '/home/jovyan/PlanetaryComputerExamples/datasets/val'\nNote dataset download directory is '/home/jovyan/PlanetaryComputerExamples/datasets'. You can update this in '/home/jovyan/.config/Ultralytics/settings.yaml'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model on the dataset for 50 epochs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/model.py:582\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    580\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[0;32m--> 582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/trainer.py:137\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myaml_file\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dataset 'dataset.yaml' error ❌ \nDataset 'dataset.yaml' images not found ⚠️, missing path '/home/jovyan/PlanetaryComputerExamples/datasets/val'\nNote dataset download directory is '/home/jovyan/PlanetaryComputerExamples/datasets'. You can update this in '/home/jovyan/.config/Ultralytics/settings.yaml'"
     ]
    }
   ],
   "source": [
    "# Train the model on the dataset for 50 epochs\n",
    "results = model.train(data='dataset.yaml', epochs=50, imgsz=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d3589b-f2a9-484e-8717-203f5c920ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val/\n",
      "    60d2de9ca164a37d24f09b7bbf178de9.png\n",
      "    60d2de9ca164a37d24f09b7bbf178de9.txt\n",
      "    6f8eb63b3ca243cd9e224ff89f1f805c.txt\n",
      "    6f8eb63b3ca243cd9e224ff89f1f805c.png\n",
      "    .ipynb_checkpoints/\n",
      "        60d2de9ca164a37d24f09b7bbf178de9-checkpoint.png\n",
      "        6f8eb63b3ca243cd9e224ff89f1f805c-checkpoint.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def print_folder_tree(start_path):\n",
    "    for root, _, files in os.walk(start_path):\n",
    "        level = root.replace(start_path, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        sub_indent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(sub_indent, f))\n",
    "\n",
    "# Use the function\n",
    "print_folder_tree('./val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1d9320f9-2a1d-4606-9e60-0029188a89ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.11.4 torch-2.2.0+cu121 CPU (Intel Xeon Platinum 8272CL 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset.yaml, epochs=100, time=None, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train50, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train50\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 133, in __init__\n",
      "    self.data = check_det_dataset(self.args.data)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/data/utils.py\", line 267, in check_det_dataset\n",
      "    file = check_file(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/utils/checks.py\", line 497, in check_file\n",
      "    raise FileNotFoundError(f\"'{file}' does not exist\")\n",
      "FileNotFoundError: 'dataset.yaml' does not exist\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/srv/conda/envs/notebook/bin/yolo\", line 8, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/cfg/__init__.py\", line 568, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/model.py\", line 582, in train\n",
      "    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/srv/conda/envs/notebook/lib/python3.11/site-packages/ultralytics/engine/trainer.py\", line 137, in __init__\n",
      "    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error ❌ {e}\")) from e\n",
      "RuntimeError: Dataset 'dataset.yaml' error ❌ 'dataset.yaml' does not exist\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train model= yolov8n.pt data= dataset.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "52fc59c1-5e5e-43b1-9886-f0e08441bcba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CODE_OF_CONDUCT.md\t\t\t\t archive\n",
      " CONTRIBUTING.md\t\t\t\t competitions\n",
      " ClubPenguin.ipynb\t\t\t\t data.yaml\n",
      " LICENSE\t\t\t\t\t dataset.yaml\n",
      "'OG Benchmark Notebook.ipynb'\t\t\t datasets\n",
      "'Phase-1 Benchmark Notebook 2024 V0.5 1.ipynb'\t docker-compose.yml\n",
      " Post_Event_Annotated_JSON\t\t\t labelme\n",
      " Post_Event_Grids_In_JPEG\t\t\t pyproject.toml\n",
      " Post_Event_Grids_In_TIFF\t\t\t quickstarts\n",
      " Post_Event_Manual_Annotated.zip\t\t runs\n",
      " Pre_Event_Grids_In_JPEG\t\t\t scripts\n",
      " Pre_Event_Grids_In_TIFF\t\t\t train\n",
      " Pre_Event_San_Juan.tif\t\t\t\t tutorials\n",
      " README.md\t\t\t\t\t val\n",
      " SECURITY.md\t\t\t\t\t yolov8n.pt\n",
      " SUPPORT.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
